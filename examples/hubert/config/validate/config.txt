(fairseq) [gry10@d3-hpc-sjtu-test-003 fairseq]$ python fairseq_cli/validate.py -h
usage: validate.py [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL] [--log-format {json,none,simple,tqdm}]
                   [--log-file LOG_FILE] [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]
                   [--tensorboard-logdir TENSORBOARD_LOGDIR] [--wandb-project WANDB_PROJECT] [--azureml-logging]
                   [--seed SEED] [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16] [--fp16]
                   [--memory-efficient-fp16] [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]
                   [--fp16-scale-window FP16_SCALE_WINDOW] [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                   [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]
                   [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp] [--amp-batch-retries AMP_BATCH_RETRIES]
                   [--amp-init-scale AMP_INIT_SCALE] [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]
                   [--empty-cache-freq EMPTY_CACHE_FREQ] [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                   [--model-parallel-size MODEL_PARALLEL_SIZE] [--quantization-config-path QUANTIZATION_CONFIG_PATH]
                   [--profile] [--reset-logging] [--suppress-crashes] [--use-plasma-view] [--plasma-path PLASMA_PATH]
                   [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubertce,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]
                   [--tokenizer {moses,nltk,space}]
                   [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
                   [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]
                   [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]
                   [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}] [--task TASK] [--num-workers NUM_WORKERS]
                   [--skip-invalid-size-inputs-valid-test] [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]
                   [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
                   [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
                   [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}] [--data-buffer-size DATA_BUFFER_SIZE]
                   [--train-subset TRAIN_SUBSET] [--valid-subset VALID_SUBSET] [--combine-valid-subsets]
                   [--ignore-unused-valid-subsets] [--validate-interval VALIDATE_INTERVAL]
                   [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
                   [--validate-after-updates VALIDATE_AFTER_UPDATES] [--fixed-validation-seed FIXED_VALIDATION_SEED]
                   [--disable-validation] [--max-tokens-valid MAX_TOKENS_VALID] [--batch-size-valid BATCH_SIZE_VALID]
                   [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]
                   [--num-shards NUM_SHARDS] [--shard-id SHARD_ID] [--grouped-shuffling]
                   [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR] [--update-ordered-indices-seed]
                   [--distributed-world-size DISTRIBUTED_WORLD_SIZE] [--distributed-num-procs DISTRIBUTED_NUM_PROCS]
                   [--distributed-rank DISTRIBUTED_RANK] [--distributed-backend DISTRIBUTED_BACKEND]
                   [--distributed-init-method DISTRIBUTED_INIT_METHOD] [--distributed-port DISTRIBUTED_PORT]
                   [--device-id DEVICE_ID] [--distributed-no-spawn]
                   [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]
                   [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]
                   [--find-unused-parameters] [--gradient-as-bucket-view] [--fast-stat-sync]
                   [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]
                   [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM] [--localsgd-frequency LOCALSGD_FREQUENCY]
                   [--nprocs-per-node NPROCS_PER_NODE] [--pipeline-model-parallel]
                   [--pipeline-balance PIPELINE_BALANCE] [--pipeline-devices PIPELINE_DEVICES]
                   [--pipeline-chunks PIPELINE_CHUNKS] [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
                   [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
                   [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
                   [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
                   [--pipeline-checkpoint {always,never,except_last}] [--zero-sharding {none,os}]
                   [--no-reshard-after-forward] [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]
                   [--not-fsdp-flatten-parameters] [--path PATH] [--post-process [POST_PROCESS]] [--quiet]
                   [--model-overrides MODEL_OVERRIDES] [--results-path RESULTS_PATH]

optional arguments:
  -h, --help            show this help message and exit
  --no-progress-bar     disable progress bar
  --log-interval LOG_INTERVAL
                        log progress every N batches (when progress bar is disabled)
  --log-format {json,none,simple,tqdm}
                        log format to use
  --log-file LOG_FILE   log file to copy metrics to.
  --aim-repo AIM_REPO   path to Aim repository
  --aim-run-hash AIM_RUN_HASH
                        Aim run hash. If skipped, creates or continues run based on save_dir
  --tensorboard-logdir TENSORBOARD_LOGDIR
                        path to save logs for tensorboard, should match --logdir of running tensorboard (default: no
                        tensorboard logging)
  --wandb-project WANDB_PROJECT
                        Weights and Biases project name to use for logging
  --azureml-logging     Log scalars to AzureML context
  --seed SEED           pseudo random number generator seed
  --cpu                 use CPU instead of CUDA
  --tpu                 use TPU instead of CUDA
  --bf16                use bfloat16; implies --tpu
  --memory-efficient-bf16
                        use a memory-efficient version of BF16 training; implies --bf16
  --fp16                use FP16
  --memory-efficient-fp16
                        use a memory-efficient version of FP16 training; implies --fp16
  --fp16-no-flatten-grads
                        don't flatten FP16 grads tensor
  --fp16-init-scale FP16_INIT_SCALE
                        default FP16 loss scale
  --fp16-scale-window FP16_SCALE_WINDOW
                        number of updates before increasing loss scale
  --fp16-scale-tolerance FP16_SCALE_TOLERANCE
                        pct of updates that can overflow before decreasing the loss scale
  --on-cpu-convert-precision
                        if set, the floating point conversion to fp16/bf16 runs on CPU. This reduces bus transfer
                        time and GPU memory usage.
  --min-loss-scale MIN_LOSS_SCALE
                        minimum FP16/AMP loss scale, after which training is stopped
  --threshold-loss-scale THRESHOLD_LOSS_SCALE
                        threshold FP16 loss scale from below
  --amp                 use automatic mixed precision
  --amp-batch-retries AMP_BATCH_RETRIES
                        number of retries of same batch after reducing loss scale with AMP
  --amp-init-scale AMP_INIT_SCALE
                        default AMP loss scale
  --amp-scale-window AMP_SCALE_WINDOW
                        number of updates before increasing AMP loss scale
  --user-dir USER_DIR   path to a python module containing custom extensions (tasks and/or architectures)
  --empty-cache-freq EMPTY_CACHE_FREQ
                        how often to clear the PyTorch CUDA cache (0 to disable)
  --all-gather-list-size ALL_GATHER_LIST_SIZE
                        number of bytes reserved for gathering stats from workers
  --model-parallel-size MODEL_PARALLEL_SIZE
                        total number of GPUs to parallelize model over
  --quantization-config-path QUANTIZATION_CONFIG_PATH
                        path to quantization config file
  --profile             enable autograd profiler emit_nvtx
  --reset-logging       when using Hydra, reset the logging at the beginning of training
  --suppress-crashes    suppress crashes when training with the hydra_train entry point so that the main method can
                        return a value (useful for sweeps)
  --use-plasma-view     Store indices and sizes in shared memory
  --plasma-path PLASMA_PATH
                        path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail.
  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubertce,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}
  --tokenizer {moses,nltk,space}
  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}
  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}
  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}
  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}
  --task TASK           task

dataset_data_loading:
  --num-workers NUM_WORKERS
                        how many subprocesses to use for data loading
  --skip-invalid-size-inputs-valid-test
                        ignore too long or too short lines in valid and test set
  --max-tokens MAX_TOKENS
                        maximum number of tokens in a batch
  --batch-size BATCH_SIZE, --max-sentences BATCH_SIZE
                        number of examples in a batch
  --required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE
                        batch size will be a multiplier of this value
  --required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE
                        maximum sequence length in batch will be a multiplier of this value
  --dataset-impl {raw,lazy,cached,mmap,fasta,huffman}
                        output dataset implementation
  --data-buffer-size DATA_BUFFER_SIZE
                        Number of batches to preload
  --train-subset TRAIN_SUBSET
                        data subset to use for training (e.g. train, valid, test)
  --valid-subset VALID_SUBSET
                        comma separated list of data subsets to use for validation (e.g. train, valid, test)
  --combine-valid-subsets, --combine-val
                        comma separated list of data subsets to use for validation (e.g. train, valid, test)
  --ignore-unused-valid-subsets
                        do not raise error if valid subsets are ignored
  --validate-interval VALIDATE_INTERVAL
                        validate every N epochs
  --validate-interval-updates VALIDATE_INTERVAL_UPDATES
                        validate every N updates
  --validate-after-updates VALIDATE_AFTER_UPDATES
                        dont validate until reaching this many updates
  --fixed-validation-seed FIXED_VALIDATION_SEED
                        specified random seed for validation
  --disable-validation  disable validation
  --max-tokens-valid MAX_TOKENS_VALID
                        maximum number of tokens in a validation batch (defaults to --max-tokens)
  --batch-size-valid BATCH_SIZE_VALID, --max-sentences-valid BATCH_SIZE_VALID
                        batch size of the validation batch (defaults to --batch-size)
  --max-valid-steps MAX_VALID_STEPS, --nval MAX_VALID_STEPS
                        How many batches to evaluate
  --curriculum CURRICULUM
                        don't shuffle batches for first N epochs
  --gen-subset GEN_SUBSET
                        data subset to generate (train, valid, test)
  --num-shards NUM_SHARDS
                        shard generation over N shards
  --shard-id SHARD_ID   id of the shard to generate (id < num_shards)
  --grouped-shuffling   shuffle batches in groups of num_shards to enable similar sequence lengths on each GPU worker
                        when batches are sorted by length
  --update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR
                        if true then prevents the reuse the epoch batch iterator by setting can_reuse_epoch_itr to
                        false, defaults to --grouped-shuffling )
  --update-ordered-indices-seed
                        if true then increment seed with epoch for getting batch iterators, defautls to False.

distributed_training:
  --distributed-world-size DISTRIBUTED_WORLD_SIZE
                        total number of GPUs across all nodes (default: all visible GPUs)
  --distributed-num-procs DISTRIBUTED_NUM_PROCS
                        total number of processes to fork (default: all visible GPUs)
  --distributed-rank DISTRIBUTED_RANK
                        rank of the current worker
  --distributed-backend DISTRIBUTED_BACKEND
                        distributed backend
  --distributed-init-method DISTRIBUTED_INIT_METHOD
                        typically tcp://hostname:port that will be used to establish initial connetion
  --distributed-port DISTRIBUTED_PORT
                        port number (not required if using --distributed-init-method)
  --device-id DEVICE_ID, --local_rank DEVICE_ID
                        which GPU to use (by default looks for $LOCAL_RANK, usually configured automatically)
  --distributed-no-spawn
                        do not spawn multiple processes even if multiple GPUs are visible
  --ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}
                        DistributedDataParallel backend
  --ddp-comm-hook {none,fp16}
                        communication hook
  --bucket-cap-mb BUCKET_CAP_MB
                        bucket size for reduction
  --fix-batches-to-gpus
                        don't shuffle batches between GPUs; this reduces overall randomness and may affect precision
                        but avoids the cost of re-reading the data
  --find-unused-parameters
                        disable unused parameter detection (not applicable to --ddp-backend=legacy_ddp)
  --gradient-as-bucket-view
                        when set to True, gradients will be views pointing to different offsets of allreduce
                        communication buckets. This can reduce peak memory usage, where the saved memory size will be
                        equal to the total gradients size. --gradient-as-bucket-view=gradient_as_bucket_view)
  --fast-stat-sync      [deprecated] this is now defined per Criterion
  --heartbeat-timeout HEARTBEAT_TIMEOUT
                        kill the job if no progress is made in N seconds; set to -1 to disable
  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as batchnorm population statistics
  --slowmo-momentum SLOWMO_MOMENTUM
                        SlowMo momentum term; by default use 0.0 for 16 GPUs, 0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6
                        for > 64 GPUs
  --slowmo-base-algorithm SLOWMO_BASE_ALGORITHM
                        Base algorithm. Either 'localsgd' or 'sgp'. Please refer to the documentation of
                        'slowmo_base_algorithm' parameter in
                        https://fairscale.readthedocs.io/en/latest/api/experimental/nn/slowmo_ddp.html for more
                        details
  --localsgd-frequency LOCALSGD_FREQUENCY
                        Local SGD allreduce frequency
  --nprocs-per-node NPROCS_PER_NODE
                        number of GPUs in each node. An allreduce operation across GPUs in a node is very fast.
                        Hence, we do allreduce across GPUs in a node, and gossip across different nodes
  --pipeline-model-parallel
                        if set, use pipeline model parallelism across GPUs
  --pipeline-balance PIPELINE_BALANCE
                        partition the model into N_K pieces, where each piece contains N_i layers. The
                        sum(args.pipeline_balance) should equal the total number of layers in the model
  --pipeline-devices PIPELINE_DEVICES
                        a list of device indices indicating which device to place each of the N_K partitions. The
                        length of this list should equal the length of the --pipeline-balance argument
  --pipeline-chunks PIPELINE_CHUNKS
                        microbatch count for pipeline model parallelism
  --pipeline-encoder-balance PIPELINE_ENCODER_BALANCE
                        partition the pipeline parallel encoder into N_K pieces, where each piece contains N_i
                        layers. The sum(args.pipeline_encoder_balance) should equal the total number of encoder
                        layers in the model
  --pipeline-encoder-devices PIPELINE_ENCODER_DEVICES
                        a list of device indices indicating which device to place each of the N_K partitions. The
                        length of this list should equal the length of the --pipeline-encoder-balance argument
  --pipeline-decoder-balance PIPELINE_DECODER_BALANCE
                        partition the pipeline parallel decoder into N_K pieces, where each piece contains N_i
                        layers. The sum(args.pipeline_decoder_balance) should equal the total number of decoder
                        layers in the model
  --pipeline-decoder-devices PIPELINE_DECODER_DEVICES
                        a list of device indices indicating which device to place each of the N_K partitions. The
                        length of this list should equal the length of the --pipeline-decoder-balance argument
  --pipeline-checkpoint {always,never,except_last}
                        checkpointing mode for pipeline model parallelism
  --zero-sharding {none,os}
                        ZeRO sharding
  --no-reshard-after-forward
                        don't reshard parameters after forward pass
  --fp32-reduce-scatter
                        reduce-scatter grads in FP32
  --cpu-offload         offload FP32 params to CPU
  --use-sharded-state   use sharded checkpoint files
  --not-fsdp-flatten-parameters
                        not flatten parameter param for fsdp

Evaluation:
  --path PATH           path(s) to model file(s), colon separated
  --post-process [POST_PROCESS], --remove-bpe [POST_PROCESS]
                        post-process text by removing BPE, letter segmentation, etc. Valid options can be found in
                        fairseq.data.utils.post_process.
  --quiet               only print final scores
  --model-overrides MODEL_OVERRIDES
                        a dictionary used to override model args at generation that were used during model training
  --results-path RESULTS_PATH
                        path to save eval results (optional)



(fairseq) [gry10@d3-hpc-sjtu-test-003 fairseq]$ python fairseq_cli/train.py -h
usage: train.py
与validate 类似

但这两个和hydra_train.txt 很不一样
得用-- xxx